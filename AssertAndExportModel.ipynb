{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix, hstack, vstack\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve\n",
    "from trainer import GridSearchCVTrainer\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Set random seed for numpy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set random seed for random\n",
    "random.seed(42)\n",
    "\n",
    "# Set random seed for os\n",
    "os.environ['PYTHONHASHSEED'] = '42'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, X_test, y_test, y_logscale=False):\n",
    "    y_pred = model.predict(X_test)\n",
    "    if(y_logscale):\n",
    "        y_pred = np.exp(y_pred)\n",
    "        \n",
    "    lines = [model_name + '\\'s evaluation results:']\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred) \n",
    "    rmse = np.sqrt(mse) \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    lines.append(f' - Mean squared error:      {mse:.2f}')\n",
    "    lines.append(f' - Root mean squared error: {rmse:.2f}')\n",
    "    lines.append(f' - Mean absolute error:     {mae:.2f}')\n",
    "    lines.append(f' - R2 error:                {r2:.2f}')\n",
    "    \n",
    "    y_pred = np.round(y_pred).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    lines.append(f' - F1 score:                {f1:.2f}')\n",
    "    lines.append(f' - Precision:               {precision:.2f}')\n",
    "    lines.append(f' - Recall:                  {recall:.2f}')\n",
    "    lines.append(f' - Accuracy:                {accuracy:.2f}')\n",
    "    lines.append('-------------------------------------------------')\n",
    "    lines.append('')\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(project_name):\n",
    "    # Import and remove NaN value\n",
    "    data_train = pd.concat([pd.read_csv('data/' + project_name + '/' + project_name + '_train.csv'),\n",
    "                        pd.read_csv('data/' + project_name + '/' + project_name + '_valid.csv')])\n",
    "    data_test = pd.read_csv('data/' + project_name + '/' + project_name + '_test.csv')\n",
    "\n",
    "    data_train['description'].replace(np.nan, '', inplace=True)\n",
    "    data_test['description'].replace(np.nan, '', inplace=True)\n",
    "\n",
    "    # Vectorize title\n",
    "    title_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "    title_vectorizer.fit(pd.concat([data_train['title'], data_test['title']]))\n",
    "\n",
    "    # Vectorize description\n",
    "    description_vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "    description_vectorizer.fit(pd.concat([data_train['description'], data_test['description']]))\n",
    "\n",
    "\n",
    "    X_train = hstack([title_vectorizer.transform(data_train['title']).astype(float),\n",
    "                    description_vectorizer.transform(data_train['description']).astype(float),\n",
    "                    data_train['title'].apply(lambda x : len(x)).to_numpy().reshape(-1, 1),\n",
    "                    data_train['description'].apply(lambda x : len(x)).to_numpy().reshape(-1, 1)\n",
    "                    ])\n",
    "\n",
    "    y_train = data_train['storypoint'].to_numpy().astype(float)\n",
    "\n",
    "    X_test = hstack([title_vectorizer.transform(data_test['title']).astype(float),\n",
    "                    description_vectorizer.transform(data_test['description']).astype(float),\n",
    "                    data_test['title'].apply(lambda x : len(x)).to_numpy().reshape(-1, 1),\n",
    "                    data_test['description'].apply(lambda x : len(x)).to_numpy().reshape(-1, 1)\n",
    "                    ])\n",
    "\n",
    "    y_test = data_test['storypoint'].to_numpy().astype(float)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'settings/BoW'\n",
    "\n",
    "# List all project names\n",
    "project_names = [name for name in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, name))]\n",
    "\n",
    "# Create a dictionary mapping setting names to models\n",
    "model_names  = ['Elastic Net', 'Support Vector Regressor', 'Random Forest Regressor', 'XGBoost Regressor', 'LightGBM Regressor']\n",
    "models = {\n",
    "    'Elastic Net': (ElasticNet(), 'Elastic Net model'),\n",
    "    'Support Vector Regressor': (SVR(), 'SVR model'),\n",
    "    'Random Forest Regressor': (RandomForestRegressor(), 'Random Forest model'),\n",
    "    'XGBoost Regressor': (XGBRegressor(), 'XGBoost Regressor model'),\n",
    "    'LightGBM Regressor': (LGBMRegressor(), 'LightGBM regressor model')\n",
    "}\n",
    "\n",
    "result_directory =  'results/BoW/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parts will train the models again and evalutation to check if the results are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project_name in project_names:\n",
    "    result_file = result_directory + project_name + '.txt'\n",
    "    with open(result_file, 'r') as f:\n",
    "        result = f.readlines()\n",
    "    result = ''.join(result)\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_data(project_name)\n",
    "\n",
    "    check_result = ''\n",
    "    trained_models = []\n",
    "    for model_name in model_names:\n",
    "        model = models[model_name][0]\n",
    "        eval_name = models[model_name][1]\n",
    "    \n",
    "        # Load the best hyperparameters\n",
    "        with open(f'settings/BoW/{project_name}/{model_name.lower()}_checkpoint.json') as f:\n",
    "            best_params = json.load(f)['best_params']\n",
    "            model.set_params(**best_params)\n",
    "        if(model.__dict__.get('n_jobs') is not None):\n",
    "            model.set_params(n_jobs=-1)\n",
    "        \n",
    "        model.fit(X_train, np.log(y_train))\n",
    "        trained_models.append(model)\n",
    "\n",
    "        check_result += '\\n'.join(evaluate_model(model, eval_name, X_test, y_test, y_logscale=True)) + '\\n'\n",
    "    \n",
    "    stack_gen = StackingCVRegressor(regressors=(trained_models[3], trained_models[4], trained_models[1], trained_models[0], trained_models[2]),\n",
    "                                    meta_regressor=trained_models[np.argmin([mean_squared_error(np.exp(model.predict(X_test)), y_test) for model in trained_models])],\n",
    "                                    use_features_in_secondary=True, n_jobs=-1, random_state=42, verbose=0)\n",
    "    stack_gen.fit(X_train, np.log(y_train))\n",
    "    check_result += '\\n'.join(evaluate_model(stack_gen, 'Stacking model', X_test, y_test, y_logscale=True)) + '\\n'\n",
    "    \n",
    "    assert check_result == result, f'{project_name}: error occured'\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print('All models are correct!')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
